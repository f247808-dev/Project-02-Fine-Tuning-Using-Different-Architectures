# -*- coding: utf-8 -*-
"""Emotion_detection_fromtext_task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MXFxnEXkDBJbzQRqnpsVpWMF_jImaxwe
"""

pip install transformers datasets evaluate torch accelerate scikit-learn

import zipfile
import os

zip_path = "/content/TASK1DATASET.zip"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("/content/emotion_data")

print("Files extracted:")
os.listdir("/content/emotion_data")

import pandas as pd

df_main = pd.read_csv("/content/emotion_data/emotions-dataset.csv")
df_labels = pd.read_csv("/content/emotion_data/emotion_labels.csv")

print("Main dataset columns:", df_main.columns)
print("Labels dataset columns:", df_labels.columns)

df_main.head()
df_labels.head()

import pandas as pd

df = pd.read_csv('/content/emotion_data/emotions-dataset.csv')
labels = pd.read_csv('/content/emotion_data/emotion_labels.csv')

print("Main dataset columns:", df.columns)
print("Labels dataset columns:", labels.columns)
df.head()

import pandas as pd
from sklearn.model_selection import train_test_split

df = df.merge(labels, left_on='sentiment', right_on='label', how='left')
df = df[['content', 'emotion']]
df.dropna(inplace=True)


emotion2id = {e: i for i, e in enumerate(df['emotion'].unique())}
id2emotion = {i: e for e, i in emotion2id.items()}
df['label'] = df['emotion'].map(emotion2id)

print("Emotion to ID mapping:", emotion2id)


train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['content'].tolist(),
    df['label'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df['label']
)

len(train_texts), len(test_texts)

from transformers import BertTokenizer
import torch
from torch.utils.data import Dataset, DataLoader

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

max_length = 128

class EmotionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            truncation=True,
            max_length=self.max_len,
            padding='max_length',
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, max_length)
test_dataset = EmotionDataset(test_texts, test_labels, tokenizer, max_length)

train_dataset[0]

from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW

from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, f1_score, classification_report
import torch
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

num_labels = len(emotion2id)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)
model = model.to(device)

epochs = 5
optimizer = AdamW(model.parameters(), lr=2e-5)
total_steps = len(train_loader) * epochs

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)


model.train()
for epoch in range(epochs):
    total_loss = 0
    loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")

    for batch in loop:
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()
        scheduler.step()

        loop.set_postfix(loss=loss.item())

    print(f"Epoch {epoch+1} Loss: {total_loss/len(train_loader)}")

import torch
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt

model.eval()

preds = []
true_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits

        predictions = torch.argmax(logits, dim=1)
        preds.extend(predictions.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

acc = accuracy_score(true_labels, preds)
f1 = f1_score(true_labels, preds, average='macro')

print("Accuracy:", acc)
print("Macro F1-score:", f1)
print("\nClassification Report:")
print(classification_report(true_labels, preds, target_names=list(emotion2id.keys())))


cm = confusion_matrix(true_labels, preds)
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=emotion2id.keys(), yticklabels=emotion2id.keys())
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

def predict_emotion(text):
    model.eval()

    encoding = tokenizer(
        text,
        return_tensors='pt',
        truncation=True,
        padding='max_length',
        max_length=128
    ).to(device)

    with torch.no_grad():
        outputs = model(**encoding)
        pred = torch.argmax(outputs.logits, dim=1).item()

    return id2emotion[pred]

test_samples = [
    "I am extremely happy today!",
    "Why does everything make me sad",
    "I am very angry right now",
    "It's just a normal day"
]

for s in test_samples:
    print(s,"=>", predict_emotion(s))

model.save_pretrained("emotion_bert_model")
tokenizer.save_pretrained("emotion_bert_model")

def predict_emotion(text):
    model.eval()

    encoding = tokenizer(
        text,
        return_tensors='pt',
        truncation=True,
        padding='max_length',
        max_length=128
    ).to(device)

    with torch.no_grad():
        outputs = model(**encoding)
        pred = torch.argmax(outputs.logits, dim=1).item()

    return id2emotion[pred]

test_samples = [
    "I am extremely happy today!",
    "Why does everything make me sad",
    "I am very angry right now",
    "It's just a normal day"
]

for s in test_samples:
    print(s,"=>", predict_emotion(s))

test_samples = [
    "I am extremely happy today!",
    "I am happiest today",
    "its fine",
    "lovely "]
for s in test_samples:
    print(s,"=>", predict_emotion(s))

