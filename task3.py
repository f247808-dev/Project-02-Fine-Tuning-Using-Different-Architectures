# -*- coding: utf-8 -*-
"""task3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ncvG1hXX_9vszNgSPPSEtTQePRGBxlzd
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d gowrishankarp/newspaper-text-summarization-cnn-dailymail

import zipfile
import os

zip_path = "newspaper-text-summarization-cnn-dailymail.zip"
extract_path = "/content/cnn_dailymail_data"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

os.listdir(extract_path)

import pandas as pd
import os

base_path = "/content/cnn_dailymail_data/cnn_dailymail"

train_df = pd.read_csv(os.path.join(base_path, "train.csv"), engine='python', on_bad_lines='skip')
val_df   = pd.read_csv(os.path.join(base_path, "validation.csv"), engine='python', on_bad_lines='skip')
test_df  = pd.read_csv(os.path.join(base_path, "test.csv"), engine='python', on_bad_lines='skip')

print("Train shape:", train_df.shape)
print("Validation shape:", val_df.shape)
print("Test shape:", test_df.shape)

train_df.head()

!pip install transformers datasets torch evaluate

import pandas as pd
import torch
from torch.utils.data import DataLoader
from transformers import BartTokenizer, BartForConditionalGeneration
from tqdm import tqdm

base_path = "/content/cnn_dailymail_data/cnn_dailymail"

train_df = pd.read_csv(f"{base_path}/train.csv", engine='python', on_bad_lines='skip')
val_df   = pd.read_csv(f"{base_path}/validation.csv", engine='python', on_bad_lines='skip')
test_df  = pd.read_csv(f"{base_path}/test.csv", engine='python', on_bad_lines='skip')

print("Train shape:", train_df.shape)
print("Validation shape:", val_df.shape)
print("Test shape:", test_df.shape)

tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

max_input_length = 512
max_output_length = 128

def encode(batch):
    inputs = tokenizer(batch['article'], truncation=True, padding="max_length", max_length=max_input_length, return_tensors="pt")
    targets = tokenizer(batch['highlights'], truncation=True, padding="max_length", max_length=max_output_length, return_tensors="pt")
    return {'input_ids': inputs.input_ids.squeeze(), 'attention_mask': inputs.attention_mask.squeeze(), 'labels': targets.input_ids.squeeze()}

subset_size = 1000
train_data = [encode(row) for i, row in train_df.head(subset_size).iterrows()]
val_data   = [encode(row) for i, row in val_df.head(200).iterrows()]

train_loader = DataLoader(train_data, batch_size=4, shuffle=True)
val_loader   = DataLoader(val_data, batch_size=4)

from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
model.train()

for epoch in range(3):
    loop = tqdm(train_loader, desc=f"Epoch {epoch+1}")
    for batch in loop:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        loop.set_postfix(loss=loss.item())

model.eval()
sample_articles = test_df['article'].head(3).tolist()

for article in sample_articles:
    inputs = tokenizer(article, return_tensors="pt", truncation=True, max_length=max_input_length).to(device)
    summary_ids = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    print("\nArticle:\n", article[:500], "...\n")
    print("Generated Summary:\n", summary)

!pip install rouge-score
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

model.eval()
all_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}

for batch in tqdm(val_loader, desc="Evaluating"):
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)


    summaries_ids = model.generate(input_ids, max_length=80, num_beams=4, early_stopping=True)
    preds = [tokenizer.decode(g, skip_special_tokens=True) for g in summaries_ids]
    refs = [tokenizer.decode(l, skip_special_tokens=True) for l in labels]

    for pred, ref in zip(preds, refs):
        scores = scorer.score(ref, pred)
        all_scores['rouge1'].append(scores['rouge1'].fmeasure)
        all_scores['rouge2'].append(scores['rouge2'].fmeasure)
        all_scores['rougeL'].append(scores['rougeL'].fmeasure)

# Compute average ROUGE scores
avg_rouge1 = sum(all_scores['rouge1']) / len(all_scores['rouge1'])
avg_rouge2 = sum(all_scores['rouge2']) / len(all_scores['rouge2'])
avg_rougeL = sum(all_scores['rougeL']) / len(all_scores['rougeL'])

print(f"ROUGE-1: {avg_rouge1:.4f}")
print(f"ROUGE-2: {avg_rouge2:.4f}")
print(f"ROUGE-L: {avg_rougeL:.4f}")

from transformers import BartTokenizer, BartForConditionalGeneration
import torch

tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
model.eval()

examples = val_df[['article', 'highlights']].head(5)

for i, row in examples.iterrows():
    article = row['article']
    reference = row['highlights']

    inputs = tokenizer(article, return_tensors="pt", max_length=1024, truncation=True).to(device)
    summary_ids = model.generate(
        inputs.input_ids,
        num_beams=4,
        max_length=100,
        early_stopping=True
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    print(f"\n--- Example {i+1} ---")
    print("Original Article:\n", article[:500], "...")  # show first 500 chars
    print("\nReference Summary:\n", reference)
    print("\nGenerated Summary:\n", summary)